{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CODE\\Lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib as mlp\n",
    "import seaborn as sns\n",
    "import re, pip, conda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Boosting PK Bagging**\n",
    "\n",
    "||装袋法 Bagging|提升法 Boosting|\n",
    "|-|-|-|\n",
    "|弱评估器|**相互独立**，并行构建|**相互关联**，按顺序依次构建<br>先建弱分类器的预测效果影响后续模型的建立|\n",
    "|建树前的抽样方式|样本有放回抽样<br>特征无放回抽样|样本有放回抽样<br>特征无放回抽样<br>先建弱分类器的预测效果可能影响抽样细节|\n",
    "|集成的结果|回归平均<br>分类众数|每个算法**具有自己独特的规则**，一般来说：<br>(1) 表现为某种分数的加权平均<br>(2) 使用输出函数|\n",
    "|目标|**降低方差**<br>提高模型整体的稳定性来提升泛化能力<br>本质是从“平均”这一数学行为中获利|**降低偏差**<br>提高模型整体的精确度来提升泛化能力<br>相信众多弱分类器叠加后可以等同于强 学习器|\n",
    "|单个评估器容易<br>过拟合的时候|具有一定的抗过拟合能力|具有一定的抗过拟合能力|\n",
    "|单个评估器的效力<br>比较弱的时候|可能失效|大概率会提升模型表现|\n",
    "|代表算法|随机森林|梯度提升树，Adaboost|\n",
    "|Weak evaluators|**Independent**, built in parallel|**Interrelated**, built in sequence<br>The prediction effect of the weak classifier built first affects the establishment of subsequent models|\n",
    "|Sampling method before tree establishment|Sample sampling with replacement<br>Feature sampling without replacement|Sample sampling with replacement<br>Feature sampling without replacement<br>The prediction effect of building a weak classifier first may affect the sampling details|\n",
    "|Integrated results|Regression average<br>Classification mode|Each algorithm**has its own unique rules**, generally speaking:<br>(1) It is expressed as a weighted average of a certain score<br>(2 ) using the output function |\n",
    "|Goal|**Reduce variance**<br>Improve the overall stability of the model to improve generalization ability<br>The essence is to profit from the mathematical behavior of \"average\"|**Reduce bias**<br>Improve Improve the generalization ability by improving the overall accuracy of the model<br>I believe that the superposition of many weak classifiers can be equivalent to a strong learner|\n",
    "|A single evaluator is prone to<br>over-fitting|Has a certain ability to resist over-fitting|Has a certain ability to resist over-fitting|\n",
    "|The effectiveness of a single estimator<br>When it is weak|may fail|high probability will improve model performance|\n",
    "|Representative Algorithm|Random Forest|Gradient Boosting Tree, Adaboost|\n",
    "\n",
    "![RF2](https://pictes.oss-cn-beijing.aliyuncs.com/%E5%BE%AE%E8%AF%BE%20-%20sklearn/RFC/RF2.png)\n",
    "\n",
    "In the Bagging algorithm represented by Random Forest, we build multiple parallel independent weak evaluators at once and let all evaluators operate in parallel. In Boosting integration algorithm, we build multiple weak evaluators (basically decision trees) one by one, and the next weak evaluator is built in a way that depends on the evaluation result of the previous weak evaluator, and finally the results of multiple weak evaluators are combined for the output, so that the weak evaluators in the Boosting algorithm are not only not independent from each other, but also strongly correlated with each other. Boosting algorithm also does not rely on the independence of weak classifiers to improve the results, which is a major difference between Boosting and Bagging. If the core difference between the different algorithms of Bagging is that they rely on different ways of achieving \"independence\" (randomness),** then the core difference between the different algorithms of Boosting is how the evaluation of the previous weak evaluator affects the creation of the next weak evaluator**.\n",
    "\n",
    "Unlike the uniform regression-to-average and classification minority-to-majority outputs of the Bagging algorithms, the Boosting algorithms exhibit a great deal of variety in their resultant outputs. While the output of early Boosting algorithms was generally the output of the last weak evaluator, the outputs of contemporary Boosting algorithms consider all of the weak evaluators in the entire integrated model. **In general, each Boosting algorithm will customise the specific form of the integration output with its own unique rules**, but for most algorithms, the output of the integration algorithm tends to be a weighted average** of some kind of result about the weak evaluators, where solving for the weights is a very critical step in the field of boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Basic Elements of Boosting Algorithms and Basic Processes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loss function $L(x,y)$: a measure of the difference between the model's predictions and the true results.\n",
    "- Weak evaluator $f(x)$: (typically) a decision tree, different boosting algorithms use different tree building processes.\n",
    "- Comprehensive integration result $H(x)$: i.e., how exactly the integration algorithm outputs the integration result\n",
    "\n",
    "**依据上一个弱评估器$f(x)_{t-1}$的结果，计算损失函数$L(x,y)$，\n",
    "    <br>并使用$L(x,y)$自适应地影响下一个弱评估器$f(x)_t$的构建。<br>集成模型输出的结果，受到整体所有弱评估器$f(x)_0$ ~ $f(x)_T$的影响.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **boosting algorithm in sklearn**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GBDT**（Gradient Boosting Decision Tree），**直方提升树**（Hist Gradient Boosting Tree）,**XGBoost**（Extreme Gradient Boosting Tree），轻量梯度提升树**LightGBM**（Light Gradiant Boosting Machine），以及离散提升树**CatBoost**（Categorial Boosting Tree）。\n",
    "\n",
    "|Boosting算法|库|集成类|\n",
    "|:--:|:--:|:--:|\n",
    "|ADB分类|sklearn|AdaBoostClassifer|\n",
    "|ADB回归|sklearn|AdaBoostRegressor|\n",
    "|梯度提升树分类|sklearn|GradientBoostingClassifier|\n",
    "|梯度提升树回归|sklearn|GradientBoostingRegressor|\n",
    "|直方提升树分类|sklearn|HistGraidientBoostingClassifier|\n",
    "|直方提升树回归|sklearn|HistGraidientBoostingRegressor|\n",
    "|极限提升树|第三方库xgboost|xgboost.train()|\n",
    "|轻量梯度提升树|第三方库lightgbm|lightgbm.train()|\n",
    "|离散提升树|第三方库catboost|catboost.train()|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### AdaBoost (Adaptive Boosting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For the first time, the result of the previous weak estimator can be adaptively affected in the subsequent modeling process<br>\n",
    "2. In the Boosting algorithm, for the first time, an output method that considers all weak evaluator results is implemented<br>\n",
    "\n",
    "As a pioneering algorithm, the construction process of AdaBoost is very simple: **First, establish a decision tree on all samples, and increase the sample weight of the incorrectly predicted samples in the data set based on the predicted results and loss function value of the decision tree. And let the weighted data set be used to train the next decision tree**. This process is equivalent to intentionally increasing the weight of \"samples that are difficult to be classified correctly\", while reducing the weight of \"samples that are easy to be classified correctly\", and directing the attention of the weak evaluator to be built subsequently to samples that are difficult to be classified correctly. on the sample.\n",
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2021MachineLearning/Ensembles/Public/boostrap-fixed2.png)\n",
    "|参数|参数含义|\n",
    "|:-:|:-:|\n",
    "|**base_estimator**|弱评估器|\n",
    "|n_estimators|集成算法中弱评估器的数量|\n",
    "|**learning_rate**|迭代中所使用的学习率|\n",
    "|**algorithm**（分类器专属）|用于指定分类ADB中使用的具体实现方法|\n",
    "|**loss**（回归器专属）|用于指定回归ADB中使用的损失函数|\n",
    "|random_state|用于控制每次建树之前随机抽样过程的随机数种子|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier as ABC\n",
    "from sklearn.ensemble import AdaBoostRegressor as ABR\n",
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "from sklearn.tree import DecisionTreeRegressor as DTR\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for classification\n",
    "data_c = load_digits()\n",
    "X_c = data_c.data\n",
    "y_c = data_c.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n",
       "       ...,\n",
       "       [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "       [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n",
       "       [ 0.,  0., 10., ..., 12.,  1.,  0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for regression\n",
    "data_r = pd.read_csv(r\"D:\\Practice\\Machine Learning\\datasets\\House Price\\train_encode.csv\",index_col=0)\n",
    "X_g = data_r.iloc[:,:-1]\n",
    "y_g = data_r.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 80)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_g.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>住宅类型</th>\n",
       "      <th>住宅区域</th>\n",
       "      <th>街道接触面积(英尺)</th>\n",
       "      <th>住宅面积</th>\n",
       "      <th>街道路面状况</th>\n",
       "      <th>巷子路面状况</th>\n",
       "      <th>住宅形状(大概)</th>\n",
       "      <th>住宅现状</th>\n",
       "      <th>水电气</th>\n",
       "      <th>...</th>\n",
       "      <th>半开放式门廊面积</th>\n",
       "      <th>泳池面积</th>\n",
       "      <th>泳池质量</th>\n",
       "      <th>篱笆质量</th>\n",
       "      <th>其他配置</th>\n",
       "      <th>其他配置的价值</th>\n",
       "      <th>销售月份</th>\n",
       "      <th>销售年份</th>\n",
       "      <th>销售类型</th>\n",
       "      <th>销售状态</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>327.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>498.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>702.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>489.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>925.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id  住宅类型  住宅区域  街道接触面积(英尺)   住宅面积  街道路面状况  巷子路面状况  住宅形状(大概)  住宅现状  水电气  \\\n",
       "0  0.0   5.0   3.0        36.0  327.0     1.0     0.0       3.0   3.0  0.0   \n",
       "1  1.0   0.0   3.0        51.0  498.0     1.0     0.0       3.0   3.0  0.0   \n",
       "2  2.0   5.0   3.0        39.0  702.0     1.0     0.0       0.0   3.0  0.0   \n",
       "3  3.0   6.0   3.0        31.0  489.0     1.0     0.0       0.0   3.0  0.0   \n",
       "4  4.0   5.0   3.0        55.0  925.0     1.0     0.0       0.0   3.0  0.0   \n",
       "\n",
       "   ...  半开放式门廊面积  泳池面积  泳池质量  篱笆质量  其他配置  其他配置的价值  销售月份  销售年份  销售类型  销售状态  \n",
       "0  ...       0.0   0.0   0.0   0.0   0.0      0.0   1.0   2.0   8.0   4.0  \n",
       "1  ...       0.0   0.0   0.0   0.0   0.0      0.0   4.0   1.0   8.0   4.0  \n",
       "2  ...       0.0   0.0   0.0   0.0   0.0      0.0   8.0   2.0   8.0   4.0  \n",
       "3  ...       0.0   0.0   0.0   0.0   0.0      0.0   1.0   0.0   8.0   0.0  \n",
       "4  ...       0.0   0.0   0.0   0.0   0.0      0.0  11.0   2.0   8.0   4.0  \n",
       "\n",
       "[5 rows x 80 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_g.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ABC(n_estimators=3).fit(X_c, y_c)\n",
    "reg = ABR(n_estimators=3).fit(X_g, y_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CODE\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:101: FutureWarning: Attribute `base_estimator_` was deprecated in version 1.2 and will be removed in 1.4. Use `estimator_` instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_depth=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(max_depth=1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.base_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeRegressor(max_depth=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor(max_depth=3)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeRegressor(max_depth=3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.base_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DecisionTreeRegressor(max_depth=3, random_state=475939812),\n",
       " DecisionTreeRegressor(max_depth=3, random_state=2109521539),\n",
       " DecisionTreeRegressor(max_depth=3, random_state=607322565)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.estimators_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `learning_rate`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Boosting ensemble method, the output $H(x)$ of the ensemble algorithm is often the weighted average of the output results of multiple weak evaluators. However, $H(x)$ is not uniformly weighted and solved after all trees are built, but is continuously calculated with iterations as the algorithm gradually builds trees. For example, for sample $x_i$, there are $T$ trees in the ensemble algorithm (that is, the value of parameter `n_estimators`). Now the $t$th weak evaluator is being established, then the $t$th weak evaluator is being established. The result of $x_i$ on the device can be expressed as $f_t(x_i)$. Assuming that the entire Boosting algorithm outputs a result of sample $x_i$ as $H(x_i)$, the result can generally be expressed as the weighted sum of all weak evaluator results in the process of t=1~t=T:\n",
    "$$H(x_i) = \\sum_{t=1}^T\\phi_tf_t(x_i)$$\n",
    "\n",
    "Among them, $\\phi_t$ is the weight of the t-th tree. For the $t$th iteration, there is:\n",
    "\n",
    "$$H_t(x_i) = H_{t-1}(x_i) + \\phi_tf_t(x_i)$$\n",
    "\n",
    "In this general process, each time the decision tree built in this round is added to the previous tree building results, the parameter $\\color{red}\\eta$ can be added in front of the weight $\\phi$, which is expressed as the tth tree added The learning rate of the overall integration algorithm, benchmarked against the parameter `learning_rate`.\n",
    "\n",
    "$$H_t(x_i) = H_{t-1}(x_i) + \\boldsymbol{\\color{red}\\eta} \\phi_tf_t(x_i)$$\n",
    "\n",
    "This learning rate parameter controls the growth rate of $H(x_i)$ during the Boosting integration process and is a very critical parameter. When the learning rate is large, $H(x_i)$ grows faster and we need fewer n_estimators. When the learning rate is small, $H(x_i)$ grows slower and we need more n_estimators. , so the boosting algorithm often needs to make a trade-off between n_estimators and learning_rate (take the XGBoost algorithm as an example).\n",
    "\n",
    "![](https://pictes.oss-cn-beijing.aliyuncs.com/%E5%BE%AE%E8%AF%BE%20-%20sklearn/week%2011%20XGBoost/eta.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - `algorithm`\n",
    ">\n",
    "> **The base_estimators that can be input by default in sklearn also need to be weak estimators that can output predicted probabilities. In actual prediction, the $H(x)$ output by AdaBoost is also aimed at the probability of a certain category**.\n",
    ">\n",
    "> In the classifier, although we are allowed to choose the algorithm, we are not allowed to choose the loss function used by the algorithm. This is because SAMME and SAMME.R use the same loss function: binary exponential loss (Exponential Loss Function) and multiple classification Multi-class Exponential loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two-class exponential loss (Exponential Loss Function) and multi-class exponential loss (Multi-class Exponential loss function).\n",
    "\n",
    "**Two classification index loss**——\n",
    "$$L(H(x),y) = e^{-yH^*(x)}$$\n",
    "Among them, y is the real classification, and $H^*(x)$ is the vector converted from the probability result $H(x)$ output by the ensemble algorithm. The conversion rules are as follows:\n",
    "\n",
    "$$H^*(x)=\n",
    "\\begin{cases}\n",
    "1& if \\ H(x)>0.5 \\\\\n",
    "-1& if\\ H(x) < 0.5\n",
    "\\end{cases}$$\n",
    "\n",
    "In sklearn, since $H(x)$ is a probability value, it needs to be converted to $H^*(x)$. If in other algorithm libraries that implement AdaBoost, the output of $H(x)$ is directly the predicted category. Then the conversion process does not need to be performed.\n",
    "\n",
    "**According to the special properties of exponential loss, the category value in the two-classification situation can only be -1 or 1**, so the value of $y$ can only be -1 or 1. When the algorithm predicts correctly, the sign of $yH^*(x)$ is positive, and the loss on the function $e^{-x}$ is very small. When the algorithm predicts incorrectly, the sign of $yH^*(x)$ is negative, and the loss on the function $e^{-x}$ is large. Binary classification exponential loss is the most classic loss function of AdaBoost. Its effectiveness in mathematical derivation and strong guidance in practice allow it to be used today.\n",
    "\n",
    "![](https://www.tf.uni-kiel.de/matwis/amat/mw1_ge/kap_5/illustr/exponential1com.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multi-category index loss**——\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(H(x),y) &=exp \\left( -\\frac{1}{K}\\boldsymbol{y^* · H^*(x)} \\right) \\\\\n",
    "& = exp \\left( -\\frac{1}{K}(y^{*1}H^{*1}(x)+y^{*2}H^{*2}(x) \\ + \\ ... + y^{*k}H^{*k}(x)) \\right)\n",
    "\\end{aligned}\n",
    "$$<br>\n",
    "Among them, $K$ is the total number of categories. For example, in the case of four categories [0,1,2,3], $K=4$, $\\boldsymbol{y^*}$ and $\\boldsymbol{H^*( x)}$ are all vectors converted according to the specific situation of multi-classification and the actual output $H(x)$ of the integration algorithm, among which $y^{*1}$ and $H^{*1}(x)$ The superscript 1 indicates the current category.\n",
    "\n",
    "In the two-classification algorithm, the algorithm will directly output the probability for one of the categories in the two-classification, because in the two-classification $P(Y=1) = 1 - P(Y=-1)$, so only Calculating the probability of a class can determine the predicted label. However, in a multi-classification algorithm, the algorithm must output probabilities for all possible value types, so that the prediction label corresponding to the maximum probability can be found. Therefore, in the ensemble algorithm, when we perform multi-class prediction, we will get the following table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.909574</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031915</td>\n",
       "      <td>0.031915</td>\n",
       "      <td>0.015957</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.003781</td>\n",
       "      <td>0.131380</td>\n",
       "      <td>0.120038</td>\n",
       "      <td>0.157845</td>\n",
       "      <td>0.134216</td>\n",
       "      <td>0.011342</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>0.163516</td>\n",
       "      <td>0.158790</td>\n",
       "      <td>0.115312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.003781</td>\n",
       "      <td>0.131380</td>\n",
       "      <td>0.120038</td>\n",
       "      <td>0.157845</td>\n",
       "      <td>0.134216</td>\n",
       "      <td>0.011342</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>0.163516</td>\n",
       "      <td>0.158790</td>\n",
       "      <td>0.115312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.092672</td>\n",
       "      <td>0.099138</td>\n",
       "      <td>0.032328</td>\n",
       "      <td>0.071121</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.370690</td>\n",
       "      <td>0.012931</td>\n",
       "      <td>0.006466</td>\n",
       "      <td>0.002155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.909574</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031915</td>\n",
       "      <td>0.031915</td>\n",
       "      <td>0.015957</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.909574  0.000000  0.010638  0.000000  0.031915  0.031915  0.015957   \n",
       "1  0.003781  0.131380  0.120038  0.157845  0.134216  0.011342  0.003781   \n",
       "2  0.003781  0.131380  0.120038  0.157845  0.134216  0.011342  0.003781   \n",
       "3  0.000000  0.092672  0.099138  0.032328  0.071121  0.312500  0.370690   \n",
       "4  0.909574  0.000000  0.010638  0.000000  0.031915  0.031915  0.015957   \n",
       "\n",
       "          7         8         9  \n",
       "0  0.000000  0.000000  0.000000  \n",
       "1  0.163516  0.158790  0.115312  \n",
       "2  0.163516  0.158790  0.115312  \n",
       "3  0.012931  0.006466  0.002155  \n",
       "4  0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = DTC(max_depth=2).fit(X_c,y_c)\n",
    "\n",
    "pd.DataFrame(clf.predict_proba(X_c)).iloc[:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row corresponds to a sample, and each column corresponds to the probability that the predicted label of the sample is a certain category. The above table is the probability table obtained by 5 samples under 10 classifications, ** and among the 10 probabilities of each sample , the category corresponding to the maximum probability is the predicted category**. This conversion can be completed by the function argmax. argmax will take out the index corresponding to the maximum value, which happens to be the prediction label corresponding to the maximum probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - `loss`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"linear\"（线性）,\"square\"（平方）,\"exponential\"（指数）\n",
    "\n",
    "**R2算法线性损失——**\n",
    "\n",
    "$$L_i = \\frac{|H(x_i) - y_i|}{D}$$\n",
    "\n",
    "**R2算法平方损失——**\n",
    "\n",
    "$$L_i = \\frac{|H(x_i) - y_i|^2}{D^2}$$\n",
    "\n",
    "**R2算法指数损失——**\n",
    "\n",
    "$$L_i = 1 - exp \\left( \\frac{-|H(x_i) - y_i|}{D} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|参数|参数含义|\n",
    "|:-:|:-:|\n",
    "|**base_estimator**|弱评估器|\n",
    "|n_estimators|集成算法中弱评估器的数量|\n",
    "|**learning_rate**|迭代中所使用的学习率|\n",
    "|**algorithm**（分类器专属）|用于指定分类ADB中使用的具体实现方法|\n",
    "|**loss**（回归器专属）|用于指定回归ADB中使用的损失函数|\n",
    "|random_state|用于控制每次建树之前随机抽样过程的随机数种子|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 1) Initialize the weight $w_i$ of the original data set, where any $w_i = \\frac{1}{M}$\n",
    "\n",
    "> - Start looping, for t in 1,2,...T:\n",
    "\n",
    "> - 2) In the existing data set $N$, there are $M$ samples with replacement sampling, forming a training set $N^t$. Every time a sample is drawn, the probability of any sample being drawn is $P_i^t = \\frac{w_i}{\\sum w_i}$. Obviously, **this probability is the current sample in the training set $N^t Weight in $**. When sampling from the initial weights, the probability $P_i^1 = \\frac{1}{M}$, and when subsequent weights change, samples with greater weights will have a greater probability of being selected. <br><br>\n",
    "> - 3) Build a regression tree $f^t$ on the training set $N^t$ according to the **CART tree** rules. The label fitted during training is the **real label**$y of the sample. ^t_i$. <br><br>\n",
    "> - 4) Input all the samples on $N^t$ into $f^t$ for prediction, and get the prediction result $f^t(x_i)$, where i = 1,2,...M. <br><br>\n",
    "> - 5) Calculate the loss function $L^t_i = L(f^t(x_i),y_i)$ on a single sample $i$. The calculation process is as follows:\n",
    ">> Solve $D = sup|f^t(x_i) - y_i|, i = 1,2,...,N$<br><br>\n",
    ">> Select one of linear/square or exponential loss functions to calculate $L^t_i$<br><br>\n",
    ">> Linear loss: $L_i = \\frac{|f^t(x_i) - y_i|}{D}$<br><br>\n",
    ">> Square loss: $L_i = \\frac{|f^t(x_i) - y_i|^2}{D^2}$<br><br>\n",
    ">> Exponential loss: $L_i = 1 - exp \\left( \\frac{-|f^t(x_i) - y_i|}{D} \\right)$<br><br>\n",
    ">> According to the requirements of AdaBoost, the value range of all losses is between [0,1].\n",
    "> - 6) Calculate the weighted average loss on the entire sample $\\bar{L^t} = \\sum_{i=1}^ML_i^tP_i^t$\n",
    ">> Note that $P_i^t$ is equal to the weight of the sample at this time. Since $P_i^t = \\frac{w_i}{\\sum w_i}$, so $P_i^t$ must be in the range [0,1], and $\\sum{P_i^t}, i=1,2, ...M$ must be 1. <br><br>\n",
    ">> **When the sum of weights is 1, the weighted average will definitely be less than or equal to the maximum value of a single value (and greater than or equal to the minimum value of a single value), so the value range of the weighted average will not exceed the value range of the single average. **. Since the range of all losses is [0,1], the range of the weighted average $\\bar{L^t}$ is also [0,1]. At the same time, since the maximum value of the loss is 1, and the maximum value of the weight $P_i^t$ must be far less than 1, the maximum value of the weighted average $\\bar{L^t}$ is generally far less than 1 of. <br>\n",
    "\n",
    "> - 7) Calculate and measure the confidence of the current integration algorithm $\\beta^t$ based on the weighted average loss $\\bar{L^t}$\n",
    ">> $\\beta^t = \\frac{\\bar{L^t}}{1-\\bar{L^t} + \\lambda}$, where $\\lambda$ is a constant to prevent the denominator from being 0<br ><br>\n",
    ">> It is not difficult to find that when the weighted average loss is very high, $\\beta^t$ is very large, so the confidence is small. When the weighted average loss is very low, $\\beta^t$ is very small, so the confidence is large. . The greater the confidence, the better the current prediction result of the ensemble algorithm. <br><br>\n",
    ">> It is known that the theoretical value range of $\\bar{L^t}$ is [0,1], so the theoretical value range of $\\beta^t$ is [0,$+\\infty$], so $\\beta_t The closer the value of $ is to 0, the better. <br><br>\n",
    ">>At the same time, we also know that the actual range of $\\bar{L^t}$ is approximately between 0.2 and 0.3, so generally speaking, the actual range of $\\beta^t$ is basically less than 1.\n",
    "> - 8) 依据置信度评估$\\beta_t$更新样本权重\n",
    ">> $w_i = w_i\\beta^{(1-L_i)}$<br><br>\n",
    ">> 我们可以根据$L_i$的范围[0,1]，以及$\\beta$的计算公式，绘制出横坐标为$L_i$，纵坐标为$\\beta^{(1-L_i)}$的图像。不难发现，**单一样本的损失越大、$\\beta^{(1-L_i)}$也会越大，因此该样本的权重会被更新得越大**。<br>\n",
    "> - 9) 求解迭代过程中弱分类器$f^t$所需的权重\n",
    ">> $\\phi^t = log(\\frac{1}{\\beta^t})$\\\n",
    ">> 其中log的底数为e或者为2皆可。当$\\beta$值越接近于0，说明损失越小、置信度越高，则$log(\\frac{1}{\\beta^t})$的值越大。所以，损失更小的树对应的权重更大，损失更大的树对应的权重更小。\n",
    "> - 10) 求解出当前迭代$t$下集成算法的输出值：\n",
    ">> $H^t(x_i) = H^{t-1}(x_i) + \\eta \\phi^t f^t(x_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 《Multi-class AdaBoost》& sklearn source code(https://github.com/scikit-learn/scikit-learn/blob/0d378913b/sklearn/ensemble/_weight_boosting.py#L913)。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
